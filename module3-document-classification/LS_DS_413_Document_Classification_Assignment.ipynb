{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S1-NLP-DS11",
      "language": "python",
      "name": "u4-s1-nlp-ds11"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "LS_DS_413_Document_Classification_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Collin-Campbell/DS-Unit-4-Sprint-1-NLP/blob/main/module3-document-classification/LS_DS_413_Document_Classification_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2u81-AdcWly"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 1, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR0YxrxZcWly"
      },
      "source": [
        "# Document Classification (Assignment)\n",
        "\n",
        "This notebook is for you to practice skills during lecture.\n",
        "\n",
        "Today's guided module project and assignment will be different. You already know how to do classification. You ready know how to extract features from documents. So? That means you're ready to combine and practice those skills in a kaggle competition. We we will open with a five minute sprint explaining the competition, and then give you 25 minutes to work. After those twenty five minutes are up, I will give a 5-minute demo an NLP technique that will help you with document classification (*and **maybe** the competition*).\n",
        "\n",
        "Today's all about having fun and practicing your skills.\n",
        "\n",
        "## Sections\n",
        "* <a href=\"#p1\">Part 1</a>: Text Feature Extraction & Classification Pipelines\n",
        "* <a href=\"#p2\">Part 2</a>: Latent Semantic Indexing\n",
        "* <a href=\"#p3\">Part 3</a>: Word Embeddings with Spacy\n",
        "* <a href=\"#p4\">Part 4</a>: Post Lecture Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8K-BYTwcWlz"
      },
      "source": [
        "# Text Feature Extraction & Classification Pipelines (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "eUEa4YH6cWlz"
      },
      "source": [
        "## Follow Along \n",
        "\n",
        "What you should be doing now:\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model (try using the pipe method I just demoed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq24mEI0cWlz"
      },
      "source": [
        "### Load Competition Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1LHlRkBiOl0",
        "outputId": "6621ae4c-3cb7-4068-f2ec-83ec8d992df6"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=c8ba46be3344089a9fdfd911ea69802b9c22e6f223d8ad7ceb8018dffb2a915f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7pwccmcl/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZN96Ql3cWl0"
      },
      "source": [
        "# https://www.kaggle.com/c/whiskey-reviews-ds20/overview\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# You may need to change the path\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGvX9eOSxaUi"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def wrangle(text):\n",
        "  stop_words = ['a','about','above','after','again','against','ain','all','along','am',\n",
        "                  'an','and','any','are','aren',\"aren't\",'as','at','be','because',\n",
        "                  'been','before','being','below','between','both','but','by','can',\n",
        "                  'couldn',\"couldn't\",'d','did','didn',\"didn't\",'do','does','doesn','doesnt'\n",
        "                  \"doesn't\",'doing','don',\"don't\",'down','during','each','every','few',\n",
        "                  'for','from','full','further','had','hadn',\"hadn't\",'has','hasn',\n",
        "                  \"hasn't\",'have','haven',\"haven't\",'having','he','her','here',\n",
        "                  'hers','herself','him','himself','his','how','i','if','in','like',\n",
        "                  'into','is','isn',\"isn't\",'it',\"it's\",\"it’s\",'its','itself','just',\n",
        "                  'll','m','ma','may','me','mightn',\"mightn't\",'more','most','mustn',\n",
        "                  \"mustn't\",'my','myself','needn',\"needn't\",'no','nose','note','notes','nor','not',\n",
        "                  'now','o','of','off','old','on','once','only','or','other','our',\n",
        "                  'ours','ourselves','out','over','own','palate','re','s','same','shan',\n",
        "                  \"shan't\",'she',\"she's\",'should',\"should've\",'shouldn',\"shouldn't\",\n",
        "                  'so','some','sometimes','such','t','than','that',\"that'll\",'the','their',\n",
        "                  'theirs','them','themselves','then','there','these','they',\n",
        "                  'this','those','through','to','too','under','until','up','ve',\n",
        "                  'very','was','wasn',\"wasn't\",'we','were','weren',\"weren't\",\n",
        "                  'what','when','where','which','while','who','whom','why','will','—',\n",
        "                  'with','won',\"won't\",'wouldn',\"wouldn't\",'y','year','yet','you',\"you'd\",\n",
        "                  \"you'll\",\"you're\",\"you've\",'your','yours','yourself','yourselves','']\n",
        "\n",
        "  # make all text lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove '\\n'\n",
        "  text = text.replace('\\n', '')\n",
        "\n",
        "  # remove punctuation\n",
        "  text = ' '.join(word.strip(string.punctuation) for word in text.split())\n",
        "\n",
        "  # remove '-'\n",
        "  text = text.replace('-', '')\n",
        "\n",
        "  cleaned_text = []\n",
        "  for item in text.split():\n",
        "    z = re.sub('[^a-z]', '', item)\n",
        "    cleaned_text.append(z)\n",
        "\n",
        "  # remove stopwords\n",
        "  super_cleaned_text = ''\n",
        "  for word in cleaned_text:\n",
        "    if word not in stop_words:\n",
        "      super_cleaned_text += (' ' + word)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return super_cleaned_text.strip()"
      ],
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx-lNeJXx_fw"
      },
      "source": [
        "train['description'] = train['description'].apply(wrangle)"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_5rCItdA3JoQ",
        "outputId": "0b96a12a-b747-4ae4-da33-b8e2c7d5840b"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>description</th>\n",
              "      <th>ratingCategory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1321</td>\n",
              "      <td>whisky batched leftover barrels returned wareh...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3861</td>\n",
              "      <td>uncommon exclusive bottling cask strength malt...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>655</td>\n",
              "      <td>release port version amruts intermediate sherr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>555</td>\n",
              "      <td>single cask aged sherry butt interacted magnif...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1965</td>\n",
              "      <td>quite herbal aromas dried tarragon parsley dil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                        description  ratingCategory\n",
              "0  1321  whisky batched leftover barrels returned wareh...               1\n",
              "1  3861  uncommon exclusive bottling cask strength malt...               0\n",
              "2   655  release port version amruts intermediate sherr...               1\n",
              "3   555  single cask aged sherry butt interacted magnif...               1\n",
              "4  1965  quite herbal aromas dried tarragon parsley dil...               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "i26ZYGgl0O3q",
        "outputId": "df8f60f6-e42a-4c96-a4ef-add40beae32f"
      },
      "source": [
        "train['description'][15]"
      ],
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fragrant spicy honeyed vanilla peaches cream butterscotch cinnamon licorice red black light nuttiness toasted oak finish nice mouthfeel wellbalanced quite rich elegant whisky'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P8M6JyKyUXt"
      },
      "source": [
        "test['description'] = test['description'].apply(wrangle)"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "kaYBAUZYdm3H",
        "outputId": "b6dd4ad6-b7d1-4c5d-d063-b777b1fb0521"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3461</td>\n",
              "      <td>style speyside single malt scotch color walnut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2604</td>\n",
              "      <td>bright lively nice balance flavors zesty fruit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3341</td>\n",
              "      <td>new olorosoforward chivas positioned split old...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3764</td>\n",
              "      <td>aged bourbon casks enhanced rioja wine casks m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2306</td>\n",
              "      <td>freshness wood laced caramel delicate minty po...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                        description\n",
              "0  3461  style speyside single malt scotch color walnut...\n",
              "1  2604  bright lively nice balance flavors zesty fruit...\n",
              "2  3341  new olorosoforward chivas positioned split old...\n",
              "3  3764  aged bourbon casks enhanced rioja wine casks m...\n",
              "4  2306  freshness wood laced caramel delicate minty po..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 355
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtFTU1QbcWl1"
      },
      "source": [
        "### Define Pipeline Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t1btktHcWl1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Creating pipeline components\n",
        "vect = TfidfVectorizer(ngram_range=(1,2))\n",
        "rfc = RandomForestClassifier()\n",
        "svd = TruncatedSVD()\n",
        "\n",
        "\n",
        "# Defining the pipeline:\n",
        "\n",
        "# LSA part of pipeline\n",
        "lsa = Pipeline([\n",
        "                # Vectorizer (i.e. data transformation)\n",
        "                ('vect', vect), \n",
        "                # TruncatedSVD\n",
        "                ('svd', svd)])\n",
        "\n",
        "# Combining into one pipeline\n",
        "pipe = Pipeline([\n",
        "                 #LSA portion\n",
        "                 ('lsa', lsa), \n",
        "                 # Classifier (in this case, random forest classifier)\n",
        "                 ('clf', rfc)\n",
        "                 ])"
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyfoGU2qjbrf",
        "outputId": "8646e2c7-e493-44f8-d376-b449c27187d3"
      },
      "source": [
        "# split target and feature matrix\n",
        "\n",
        "X = train['description']\n",
        "y = train['ratingCategory']\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "assert len(X) == len(y)"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4087,)\n",
            "(4087,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZ8FDStcWl1"
      },
      "source": [
        "### Define Your Search Space\n",
        "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmA8c8uacWl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd7afae-558d-409a-caf7-e0ae254f033e"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {\n",
        "    'lsa__svd__n_components': [10, 100, 250],\n",
        "    'lsa__vect__max_df': (0.7,  0.8),\n",
        "    'lsa__vect__min_df': (0.1, 0.2),\n",
        "    'lsa__vect__max_features': (500, 1000),\n",
        "    'clf__max_depth':(5,10,15,20),\n",
        "    'clf__n_estimators':[10, 100, 250]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe,\n",
        "                           parameters, \n",
        "                           cv=4, \n",
        "                           n_jobs=-1, \n",
        "                           verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "grid_search.fit(X, y)"
      ],
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 288 candidates, totalling 1152 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   23.0s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  7.3min\n",
            "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed: 10.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=4, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('lsa',\n",
              "                                        Pipeline(memory=None,\n",
              "                                                 steps=[('vect',\n",
              "                                                         TfidfVectorizer(analyzer='word',\n",
              "                                                                         binary=False,\n",
              "                                                                         decode_error='strict',\n",
              "                                                                         dtype=<class 'numpy.float64'>,\n",
              "                                                                         encoding='utf-8',\n",
              "                                                                         input='content',\n",
              "                                                                         lowercase=True,\n",
              "                                                                         max_df=1.0,\n",
              "                                                                         max_features=None,\n",
              "                                                                         min_df=1,\n",
              "                                                                         ngram_range=(1,\n",
              "                                                                                      2),\n",
              "                                                                         norm='l2',\n",
              "                                                                         preprocessor=None,\n",
              "                                                                         smooth_...\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'clf__max_depth': (5, 10, 15, 20),\n",
              "                         'clf__n_estimators': [10, 100, 250],\n",
              "                         'lsa__svd__n_components': [10, 100, 250],\n",
              "                         'lsa__vect__max_df': (0.7, 0.8),\n",
              "                         'lsa__vect__max_features': (500, 1000),\n",
              "                         'lsa__vect__min_df': (0.1, 0.2)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 358
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6pc1AVCMiW2",
        "outputId": "82859ef0-7721-4318-af9e-254f33753bc2"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf__max_depth': 5,\n",
              " 'clf__n_estimators': 250,\n",
              " 'lsa__svd__n_components': 10,\n",
              " 'lsa__vect__max_df': 0.7,\n",
              " 'lsa__vect__max_features': 1000,\n",
              " 'lsa__vect__min_df': 0.1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 359
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhhEELD9Mfof",
        "outputId": "17ca9b2f-d834-4c1e-d573-5b904cf8e777"
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7085878546607352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 361
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koYL_W67cWl2"
      },
      "source": [
        "### Make a Submission File\n",
        "*Note:* In a typical Kaggle competition, you are only allowed two submissions a day, so you only submit if you feel you cannot achieve higher test accuracy. For this competition the max daily submissions are capped at **20**. Submit for each demo and for your assignment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL6RCvTscWl2"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = grid_search.predict(test['description'])"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8OOMTWEcWl2"
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
        "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSvboWJ8cWl2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8c00885b-8468-43c2-c47c-5007e565d394"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission.head()"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>ratingCategory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3461</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2604</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3341</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3764</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2306</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ratingCategory\n",
              "0  3461               1\n",
              "1  2604               1\n",
              "2  3341               1\n",
              "3  3764               1\n",
              "4  2306               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4VV4OL_cWl3"
      },
      "source": [
        "subNumber = 5\n",
        "\n",
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "\n",
        "submission.to_csv(f'submission{subNumber}.csv', index=False)\n",
        "subNumber += 1"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5bta_2OcWl3"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You're trying to achieve a minimum of 80% Accuracy on your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HoZCw8UcWl3"
      },
      "source": [
        "## Latent Semantic Indexing (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "KHvm7ZgzcWl3"
      },
      "source": [
        "## Follow Along\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model & try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
        "4. Make a submission to Kaggle \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJNIBG2NcWl3"
      },
      "source": [
        "### Define Pipeline Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7bdJ0U4cWl3"
      },
      "source": [
        "# Completed above!!!!!!!!\n",
        "lsi = ...\n",
        "vect = ...\n",
        "clf = ...\n",
        "\n",
        "pipe = Pipeline([('lsi', lsi), ('clf', clf)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFLa8hJfcWl3"
      },
      "source": [
        "### Define Your Search Space\n",
        "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gBRcGPxcWl3"
      },
      "source": [
        "parameters = {\n",
        "    'lsi__svd__n_components': [10,100,250],\n",
        "    'lsi__vect__max_df': (0.75, 1.0),\n",
        "    'clf__max_depth':(5,10,15,20)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=4, verbose=1)\n",
        "grid_search.fit(..., ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx3jzm-scWl3"
      },
      "source": [
        "### Make a Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCvqJrdecWl3"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = grid_search.predict(test['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmium6wqcWl4"
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
        "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc9eFvnJcWl4"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e--po0EMcWl4"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "\n",
        "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
        "subNumber += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83rMTLiUcWl4"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Continue to apply Latent Semantic Indexing (LSI) to various datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjCbEubvcWl4"
      },
      "source": [
        "# Word Embeddings with Spacy (Learn)\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr5lmB_7cWl4"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTVIq7xlcWl4"
      },
      "source": [
        "# Apply to your Dataset\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_dist = {\n",
        "    \n",
        "    'max_depth' : randint(3,10),\n",
        "    'min_samples_leaf': randint(2,15)\n",
        "}"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCbOrcHbcWl4"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOgLyVbAjMoo"
      },
      "source": [
        "# Function to return the vector for each sentence in a document\n",
        "def get_word_vectors(docs):\n",
        "    return [nlp(doc).vector for doc in docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtaampYFjRYt"
      },
      "source": [
        "X_train = get_word_vectors(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ZByqpYjdtn"
      },
      "source": [
        "classifier = RandomForestClassifier()\n",
        "\n",
        "classifier.fit(X_train,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0kC-fFpcWl4"
      },
      "source": [
        "### Make a Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFFZSIFecWl4"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = ...predict(test['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aM_nA_bcWl5"
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
        "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl3ghJ1NcWl5"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUpW0CRtcWl5"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "\n",
        "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
        "subNumber += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvpcux0YcWl5"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "What you should be doing now:\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model & try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
        "    - Try to extract word embeddings with Spacy and use those embeddings as your features for a classification model.\n",
        "4. Make a submission to Kaggle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEzSAQb6cWl5"
      },
      "source": [
        "# Post Lecture Assignment\n",
        "<a id=\"p4\"></a>\n",
        "\n",
        "Your primary assignment this afternoon is to achieve a minimum of 80% accuracy on the Kaggle competition. Once you have achieved 70% accuracy, please work on the following: \n",
        "\n",
        "1. Research \"Sentiment Analysis\". Provide answers in markdown to the following questions: \n",
        "    - What is \"Sentiment Analysis\"? \n",
        "      *   Sentiment Analysis is when one uses text analysis techniques in order to analyze the emotions being conveyed within text data.\n",
        "\n",
        "    - Is Document Classification different than \"Sentiment Analysis\"? Provide evidence for your response\n",
        "      *   Sentiment Analysis is a type of Document Classification.  For example, we could classify a document by its sentiment or by other classes such as subject type, etc.\n",
        "\n",
        "    - How do create labeled sentiment data? Are those labels really sentiment?\n",
        "      *   Define clear rules for each label.  The labels are, of course, subjective.\n",
        "\n",
        "    - What are common applications of sentiment analysis?\n",
        "      *   Customer reviews, restaurant reviews, consumer reviews...\n",
        "\n",
        "2. Research why word embeddings worked better for the lecture notebook than on the whiskey competition.\n",
        "    - This [text classification documentation](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) from Google might be of interest\n",
        "    - Neural Networks are becoming more popular for document classification. Why is that the case?\n",
        "      *   Word embeddings require large amounts of text data, and Neural Networks allow for more efficiency in this aspect. "
      ]
    }
  ]
}